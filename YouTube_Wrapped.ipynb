{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YouTube Wrapped.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WZFdGFOKHZh"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "import json\n",
        "import google_auth_oauthlib.flow\n",
        "from googleapiclient.discovery import build\n",
        "import googleapiclient.errors\n",
        "\n",
        "import pandas as pd\n",
        "import isodate\n",
        "\n",
        "import urllib.parse "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# configure YouTube API\n",
        "api_service_name = \"youtube\"\n",
        "api_version = \"v3\"\n",
        "api_key = \"GET_API_KEY\"\n",
        "\n",
        "# Get credentials and create an API client\n",
        "youtube = build(\n",
        "    api_service_name, api_version, developerKey=api_key)"
      ],
      "metadata": {
        "id": "KQnZx77SL0TE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data extraction\n",
        "\n",
        "\n",
        "#create empty DF\n",
        "df = pd.DataFrame() \n",
        "\n",
        "#Open YouTube takeout file and convert to BS\n",
        "with open(\"/TakeOut/YouTube and YouTube Music/history/watch-history.html\") as html:\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "rows = soup.find_all(\"div\", {\"class\": \"outer-cell\"})\n",
        "\n",
        "for row in rows:\n",
        "  content_data = row.find(\"div\", {\"class\": \"content-cell\"})\n",
        "  date_watched = str(content_data).split(\"<br/>\")[-1][:-6]\n",
        "  item_data = row.find_all('a')\n",
        "  if len(item_data) > 0:  \n",
        "    title = item_data[0].get_text()\n",
        "    link = item_data[0]['href']\n",
        "    url = urllib.parse.urlparse(link)\n",
        "    params = urllib.parse.parse_qs(urllib.parse.urlparse(link).query)\n",
        "\n",
        "    # set default to overwrite\n",
        "    channel = \"NA\"\n",
        "    tags = \"NA\"\n",
        "    duration = 0\n",
        "\n",
        "    if(params):\n",
        "      # small check so it won't try to make a request for empty data\n",
        "      youtubeId = params[\"v\"][0]\n",
        "\n",
        "    if(youtubeId):\n",
        "      # small check so it won't try to make a request for empty data\n",
        "\n",
        "      # make youtube API request to get video informatin\n",
        "      # note that you make a single request for every individual video\n",
        "      request = youtube.videos().list(\n",
        "        part=\"snippet,contentDetails\",\n",
        "        id=youtubeId\n",
        "        )\n",
        "      response = request.execute()\n",
        "      if(len(response['items']) > 0):\n",
        "        # note: duration is in SECONDS\n",
        "        duration = isodate.parse_duration(response['items'][0]['contentDetails']['duration']).total_seconds()\n",
        "\n",
        "        if('tags' in response['items'][0]['snippet']):\n",
        "          tags = response['items'][0]['snippet']['tags']\n",
        "\n",
        "    #add data to DF\n",
        "    rowJson = [{\n",
        "      'title': title,\n",
        "      'link': link,\n",
        "      'dateWatched': date_watched,\n",
        "      'channel': channel,\n",
        "      'tags': tags,\n",
        "      'duration': duration\n",
        "    }]\n",
        "    rowDf = pd.DataFrame(data=rowJson)\n",
        "    df = pd.concat([df, rowDf])\n",
        "\n",
        "# you can export your data to a CSV or TSV for manually analysis, or to save some time with the requests later\n",
        "# df.to_csv(\"durationAndTags.csv\")"
      ],
      "metadata": {
        "id": "kjyGKIc_MULI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## data anlysis\n",
        "\n",
        "# FROM TAGS TO CATEGORIES\n",
        "\n",
        "# Check your tags manualy and check which words could classify categories\n",
        "# Note, that the order is important; as videos can have multiple tags, the last line will always prevail\n",
        "\n",
        "# Some examples\n",
        "df.loc[(df.tags.str.contains(\"meditation\"), 'category')]='meditation'\n",
        "df.loc[(df.tags.str.contains(\"Y Combinator\"), 'category')]='YC'\n",
        "df.loc[(df.tags.str.contains(\"chinese\"), 'category')]='Chinese'\n",
        "\n",
        "# FUNCTIONS\n",
        "\n",
        "# get minutes watched by category\n",
        "def get_minutes(category):\n",
        "  allCatVids = df.loc[df['category'] == category]\n",
        "  totalWatched = str(round(allCatVids['duration'].sum() / 60, 1)) + \"min\"\n",
        "  return totalWatched\n",
        "\n",
        "def get_count(category):\n",
        "  allCatVids = df.loc[df['category'] == category]\n",
        "  return str(len(allCatVids)) + \" videos\"\n",
        "\n",
        "def get_summary(category):\n",
        "  return \"Of the category \" + category + \", you watched \" + get_count(category) + \", with a total of \" + get_minutes(category) + \" watched.\"\n",
        "\n",
        "# CLEAN UP DATA SET\n",
        "# remove duplicates from certain categories\n",
        "# some categories, you want to only count the \"unique videos\"\n",
        "def dropDuplicates(categoryList, dataframe):\n",
        "  for category in categoryList:\n",
        "    dataframe.loc[(dataframe.category == category)] = dataframe.loc[(dataframe.category == category)].drop_duplicates(subset=['link'])\n",
        "\n",
        "singleEntryCategories = ['YC', 'Chinese']\n",
        "dropDuplicates(singleEntryCategories, df)\n",
        "\n",
        "g = round(df.groupby(['channel'])['duration'].sum() / 60,  1)\n",
        "j = df.groupby(['channel']).size().to_frame('count')\n",
        "channelDf = pd.merge(g, j, left_index=True, right_index=True).reset_index().sort_values(by=['count'], ascending=False)\n",
        "\n",
        "wantToKnowCats = [\"Chinese\", \"meditation\"]\n",
        "for cat in wantToKnowCats:\n",
        "  print(get_summary(cat))\n"
      ],
      "metadata": {
        "id": "uEJNhqiKPXjy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}